# ViT_vs_CNN_Adversarial_Attacks

## Youtube Video Link for Presentation

Video: https://youtu.be/5BSIlUJaB4k

## Important Files Included: 

- ViT_vs_CNN_Adversarial_Attacks.ipynb: A jupyter notebook that contains all the process code with annotations and explainations. All the discussions and evaluations are also included in this notebook.
- ViT_vs_CNN_Adversarial_Attacks_Poster: The poster products files are included in this folder. They are also displayed in this readme section `Poster Product Display.`

## Description
This final project examines the comparative performance of Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) in the context of adversarial attacks. CNNs, which have long dominated computer vision tasks, are well-documented for their vulnerability to adversarial examples. The subtle perturbations of adversarial attacks cause models to misclassify inputs with high confidence. ViTs, a more recent architecture leveraging self-attention mechanisms, have shown promise in achieving competitive or superior results in various vision tasks. However, their behavior under adversarial conditions, especially compared to CNNs, remains less thoroughly studied. This project aims to provide insights into their performance differences when subjected to adversarial perturbations, focusing on accuracy as the primary evaluation metric. 

Furthermore, explainable Artificial Intelligence techiniques such as saliency map is applied to visualize whether the two models rely on different features in predicting and labeling the adversarial examples. Understanding the model prediction process is important as explainability is becoming increasingly crucial especially when dealing with impactful decisions in areas like healthcare, finance, and criminal justice. Explainability enhances model interpretability, allowing users to understand and explain AI decisions, while also addressing regulatory requirements that demand transparency and accountability. Moreover, it fosters trust and adoption of AI systems, supports model validation by detecting biases and errors, and aids in debugging and improvement, making it easier to identify and correct issues in the decision-making process 

## Background / Literature Review

The findings in "On the Robustness of Vision Transformers to Adversarial Examples" by Kaleel Mahmood et al., seeks to address gaps in understanding adversarial performance differences between CNNs and ViTs. Research has shown that CNNs are often highly transferable in adversarial examples, which means that the adversarial attacks designed for one CNN often succeed on others. On the other hand, ViTs exhibit distinct properties, with adversarial examples showing lower transferability between ViTs and CNNs. 

Moreover, CNNs and ViTs differ fundamentally in their approach to processing images, with CNNs relying on spatially localized convolutional filters and ViTs leveraging global self-attention mechanisms. These architectural differences motivate a deeper exploration of how each model performs under adversarial scenarios. 

## Extended Motivations

An additional motivations in generating Adversarial Attacks using Vision Transformers and Convolutional Neural Network Models lies in the fact that my best friend is a fashion designer who often struggles with protecting her own copyright designs as well as coming up with creative notions to her designed costumes. This knowledge of generating adversarial attacks provide alternative tracks and perspectives for designs while securing her works to AI technologies. 

## Poster Product Display

The same poster files can be found in the `ViT_vs_CNN_Adversarial_Attacks_Poster` Folder

![Poster #1](https://github.com/user-attachments/assets/34f1fc3e-c96a-4d15-adab-164f498c9e8f)

![Poster #2](https://github.com/user-attachments/assets/03850642-fc26-4a06-961b-016a25e43e07)

![Poster #3](https://github.com/user-attachments/assets/573e3452-84a1-4f07-a292-b4dd4fa06162)

![Poster #4](https://github.com/user-attachments/assets/350125ec-af73-4595-8751-896d7005c073)

![Poster #5](https://github.com/user-attachments/assets/2c4a9a2c-0ad8-470e-90e1-1cb760a14729)

